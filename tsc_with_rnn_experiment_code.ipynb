{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ucr_ts_datasets_dir = \"../datasets/UCR_TS_Archive_2015/\"\n",
    "base_results_path=\"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "SEED = 813306\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "import keras.layers as L \n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_names = [fname for fname in os.listdir(ucr_ts_datasets_dir)\n",
    "                 if fname[0] != \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A set of methods for running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_ucr_dataset(dataset_name, \n",
    "                     base_path=ucr_ts_datasets_dir,\n",
    "                     convert_labels_to_categorical=True,\n",
    "                     normalize=True):\n",
    "    \n",
    "    train_filepath = os.path.join(base_path, dataset_name, dataset_name + \"_TRAIN\")\n",
    "    train = np.loadtxt(train_filepath, delimiter=\",\")\n",
    "    y_train = train[:,0]\n",
    "    x_train = train[:,1:]\n",
    "    \n",
    "    test_filepath = os.path.join(base_path, dataset_name, dataset_name + \"_TEST\")\n",
    "    test = np.loadtxt(test_filepath, delimiter=\",\")\n",
    "    y_test = test[:,0]\n",
    "    x_test = test[:,1:]\n",
    "    \n",
    "    if convert_labels_to_categorical:\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        y_max = y_train.max()\n",
    "        y_min = y_train.min()\n",
    "        \n",
    "        # Convert labels to range 0. to (num_classes - 1)\n",
    "        y_train = (y_train - y_min) / (y_max - y_min) * (num_classes - 1)\n",
    "        y_test =  (y_test  - y_min) / (y_max - y_min) * (num_classes - 1)\n",
    "        \n",
    "        y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "        y_test =  np_utils.to_categorical(y_test,  num_classes)\n",
    "        \n",
    "    if normalize:\n",
    "        # Do not leak information about test data \n",
    "        x_train_mean = x_train.mean()\n",
    "        x_train_stdev = x_train.std()\n",
    "        \n",
    "        x_train = (x_train - x_train_mean) / x_train_stdev\n",
    "        x_test =  (x_test  - x_train_mean) / x_train_stdev\n",
    "    \n",
    "    return ((x_train, y_train), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nn_and_save_results(dataset_name, model_name,\n",
    "                        x_train, y_train, \n",
    "                        x_test, y_test,\n",
    "                        batch_size,\n",
    "                        model, \n",
    "                        opt_params,\n",
    "                        base_path=base_results_path,\n",
    "                        load_weights_if_exist=True):\n",
    "    \n",
    "    result_path = os.path.join(base_path, model_name, dataset_name)\n",
    "    file_prefix = \"%s_%s\" %(model_name, dataset_name)\n",
    "    \n",
    "    weights_filename = os.path.join(result_path, file_prefix+\"_weights.h5\")\n",
    "    history_filename = os.path.join(result_path, file_prefix+\"_history.csv\")\n",
    "    log_filename = os.path.join(result_path, file_prefix+\"_fit_log.csv\")\n",
    "    \n",
    "    initial_epoch = 0\n",
    "    \n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    elif load_weights_if_exist: #Continue learning from existing weights file\n",
    "        model.load_weights(weights_filename)\n",
    "        initial_epoch = len(pd.read_csv(log_filename))\n",
    "  \n",
    "    model.compile(loss=opt_params.get(\"loss\", \"categorical_crossentropy\"), \n",
    "                  optimizer=opt_params.get(\"optimizer\", \"Adam\"),\n",
    "                  metrics=opt_params.get(\"metrics\", [\"accuracy\"]))  \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=opt_params.get(\"reduce_lr_monitor\", \"loss\"),              \n",
    "        factor=opt_params.get(\"reduce_lr_factor\", 0.5),\n",
    "        patience=opt_params.get(\"reduce_lr_patience\", 200), \n",
    "        min_lr=opt_params.get(\"reduce_lr_min_lr\", 0.01))   \n",
    "       \n",
    "    model_learning_history = model.fit(\n",
    "        x_train, y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=opt_params.get(\"epochs\", 500),\n",
    "        verbose=0, \n",
    "        validation_data=(x_test, y_test), \n",
    "        initial_epoch=initial_epoch,\n",
    "        callbacks = [\n",
    "            reduce_lr, \n",
    "            TQDMNotebookCallback(), \n",
    "            ModelCheckpoint(\n",
    "                os.path.join(\n",
    "                    result_path, \n",
    "                    file_prefix)+\\\n",
    "                    \"_weights_checkpoint_{epoch:02d}-{val_loss:.2f}.h5\", \n",
    "                period=50),\n",
    "            CSVLogger(log_filename, append=True)])    \n",
    "    #Save weights\n",
    "    model.save(weights_filename)\n",
    "    #Save learning history\n",
    "    pd.DataFrame(model_learning_history.history)\\\n",
    "        .to_csv(history_filename)\n",
    "    \n",
    "    #Save predicted train classes\n",
    "    pd.DataFrame(\n",
    "        {\"y_true\":      np.argmax(y_train, axis=1), \n",
    "         \"y_predicted\": np.argmax(model.predict(x_train), axis=1)})\\\n",
    "        .to_csv(os.path.join(result_path, \n",
    "                             file_prefix+\"_train_labels_and_preds.csv\"))\n",
    "    \n",
    "    #Save predicted test classes\n",
    "    pd.DataFrame(\n",
    "        {\"y_true\":      np.argmax(y_test, axis=1), \n",
    "         \"y_predicted\": np.argmax(model.predict(x_test), axis=1)})\\\n",
    "        .to_csv(os.path.join(result_path, \n",
    "                             file_prefix+\"_test_labels_and_preds.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_experiments_on_datasets(model_name, model_creation_function, \n",
    "                                model_opt_params, add_dim=False, \n",
    "                                dataset_names=dataset_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "                 A name of the studied model. Will be used as a folder name and as a prefix for resulting files.\n",
    "    \n",
    "    model_creation_function : (ts_length : int, num_classes : int) -> keras.Model \n",
    "                              A function that creates a Keras model using the values of input time series length and the number of classes.\n",
    "    \n",
    "    model_opt_params : dict\n",
    "                       A dictionary which contains the value of optimization parameters for Keras' 'model.compile' method and learning callbacks. For example, 'optimizer' or 'epochs'. \n",
    "    \n",
    "    add_dim : boolean (default False)\n",
    "              Shows if it is necessary to add additional dummy dimension of size 1 to input tensor.\n",
    "    \n",
    "    dataset_names : list of str\n",
    "                    A list of names of UCR Datasests which will be used for the experiment.\n",
    "    \n",
    "    \"\"\"\n",
    "    for dataset_name in dataset_names:\n",
    "        (x_train, y_train), (x_test, y_test) = read_ucr_dataset(dataset_name)\n",
    "        batch_size = min(int(x_train.shape[0]/10), 16)\n",
    "        \n",
    "        model = model_creation_function(x_train.shape[1], y_train.shape[1])\n",
    "        \n",
    "        if add_dim:\n",
    "            x_train = x_train.reshape(x_train.shape + (1,))\n",
    "            x_test = x_test.reshape(x_test.shape + (1,))\n",
    "        \n",
    "        train_nn_and_save_results(dataset_name, model_name,\n",
    "                                  x_train, y_train, \n",
    "                                  x_test, y_test,\n",
    "                                  batch_size, \n",
    "                                  model, \n",
    "                                  model_opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generation functions and the defenition of learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(ts_length, num_classes):\n",
    "    mlp = Sequential()\n",
    "    \n",
    "    mlp.add(L.Dropout(0.1, input_shape=(ts_length,)))\n",
    "    mlp.add(L.Dense(500, activation=\"relu\"))\n",
    "    \n",
    "    mlp.add(L.Dropout(0.2))\n",
    "    mlp.add(L.Dense(500, activation=\"relu\"))\n",
    "    \n",
    "    mlp.add(L.Dropout(0.2))\n",
    "    mlp.add(L.Dense(500, activation=\"relu\"))\n",
    "    \n",
    "    mlp.add(L.Dropout(0.3))\n",
    "    mlp.add(L.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "    return mlp\n",
    "\n",
    "mlp_opt_params = {\n",
    "    \"optimizer\": \"adadelta\",\n",
    "    \"reduce_lr_patience\": 200,\n",
    "    \"reduce_lr_min_lr\": 0.01,\n",
    "    \"epochs\": 5000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fcn_model(ts_length, num_classes):\n",
    "    fcn = Sequential()\n",
    "    \n",
    "    fcn.add(L.Conv1D(128, 8, padding=\"same\", \n",
    "                     input_shape=(ts_length,1,)))\n",
    "    fcn.add(L.normalization.BatchNormalization())\n",
    "    fcn.add(L.Activation(\"relu\"))\n",
    "    \n",
    "    fcn.add(L.Conv1D(256, 5, padding=\"same\"))\n",
    "    fcn.add(L.normalization.BatchNormalization())\n",
    "    fcn.add(L.Activation(\"relu\"))\n",
    "    \n",
    "    fcn.add(L.Conv1D(128, 3, padding=\"same\"))\n",
    "    fcn.add(L.normalization.BatchNormalization())\n",
    "    fcn.add(L.Activation(\"relu\"))\n",
    "    \n",
    "    fcn.add(L.GlobalAveragePooling1D())\n",
    "    fcn.add(L.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "    return fcn\n",
    "\n",
    "fcn_opt_params = {\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"reduce_lr_min_lr\": 0.0001,\n",
    "    \"epochs\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_simple_rnn_rnn_model(ts_length, num_classes, \n",
    "                               layers=[128, 128, 128]):\n",
    "    rnn = Sequential()\n",
    "    rnn.add(L.SimpleRNN(layers[0], \n",
    "                        input_shape=(None, 1,),\n",
    "                        activation=\"tanh\", \n",
    "                        return_sequences=True))\n",
    "    if len(layers) > 1:\n",
    "        for layer_width in layers[1:]:\n",
    "            rnn.add(L.SimpleRNN(layer_width, \n",
    "                    activation=\"tanh\", \n",
    "                    return_sequences=True))\n",
    "\n",
    "    rnn.add(L.SimpleRNN(num_classes, activation=\"softmax\"))\n",
    "    return rnn\n",
    "\n",
    "simple_rnn_opt_params = {\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"reduce_lr_patience\": 10,\n",
    "    \"reduce_lr_min_lr\": 0.01,\n",
    "    \"epochs\": 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnn_dense_model(ts_length, num_classes, \n",
    "                                 layers=[128, 128, 128]):\n",
    "    \n",
    "    more_than_one_hidden = len(layers) > 1\n",
    "    \n",
    "    rnn = Sequential()\n",
    "    rnn.add(L.SimpleRNN(layers[0], \n",
    "                        input_shape=(None, 1,),\n",
    "                        activation=\"tanh\", \n",
    "                        return_sequences=more_than_one_hidden))\n",
    "    l = 1\n",
    "    if more_than_one_hidden:\n",
    "        for layer_width in layers[1:]:\n",
    "            l += 1\n",
    "            rnn.add(L.SimpleRNN(layer_width, \n",
    "                    activation=\"tanh\", \n",
    "                    return_sequences=(l!=len(layers))))\n",
    "            \n",
    "\n",
    "    rnn.add(L.Dense(num_classes, activation=\"softmax\"))\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(ts_length, num_classes, \n",
    "                     layers=[128, 128, 128]):\n",
    "    more_than_one_hidden = len(layers) > 1\n",
    "    \n",
    "    rnn = Sequential()\n",
    "    rnn.add(L.LSTM(layers[0], \n",
    "                        input_shape=(None, 1,),\n",
    "                        activation=\"tanh\", \n",
    "                        return_sequences=more_than_one_hidden))\n",
    "    l = 1\n",
    "    if more_than_one_hidden:\n",
    "        for layer_width in layers[1:]:\n",
    "            l += 1\n",
    "            rnn.add(L.LSTM(layer_width, \n",
    "                    activation=\"tanh\", \n",
    "                    return_sequences=(l!=len(layers))))\n",
    "            \n",
    "\n",
    "    rnn.add(L.Dense(num_classes, activation=\"softmax\"))\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments code execution\n",
    "\n",
    "##### Feedforward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"mlp\", build_mlp_model, mlp_opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"fcn\", build_fcn_model, fcn_opt_params, add_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The networks with simple recurrent hidden layers and a dense output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_dense\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_dense_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_128_dense\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_dense_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128, 128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_128_128_dense\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_dense_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128, 128, 128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_256_dense\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_dense_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[256]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The networks with LSTM hidden layers and a dense output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"lstm_128_dense\", \n",
    "    lambda ts_length, num_classes: build_lstm_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"lstm_128_128_dense\", \n",
    "    lambda ts_length, num_classes: build_lstm_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128, 128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"lstm_256_dense\", \n",
    "    lambda ts_length, num_classes: build_lstm_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[256]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The networks with simple recurrent hidden layers and a recurrent output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_rnn\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_rnn_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_128_rnn\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_rnn_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128, 128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiments_on_datasets(\n",
    "    \"rnn_128_128_rnn\", \n",
    "    lambda ts_length, num_classes: build_simple_rnn_rnn_model(\n",
    "                                       ts_length, num_classes, \n",
    "                                       layers=[128, 128, 128]), \n",
    "    simple_rnn_opt_params, \n",
    "    add_dim=True, \n",
    "    dataset_names=dataset_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
